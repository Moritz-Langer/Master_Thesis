@dataset{AgrarmarktAustriaINVEKOSSchlageOsterreich2024,
  title = {{{INVEKOS Schläge Österreich}} 2024},
  author = {AgrarmarktAustria},
  date = {2024},
  url = {https://geoportal.inspire.gv.at/metadatensuche/inspire/api/records/bbd8ee1b-76a1-451f-88ef-84e9c7a1faaf},
  keywords = {/unread}
}

@inproceedings{AudebertEtAlJointLearningEarthObservationOpenStreetMapDataGetFasterBetterSemanticMaps2017,
  title = {Joint {{Learning}} from {{Earth Observation}} and {{OpenStreetMap Data}} to {{Get Faster Better Semantic Maps}}},
  author = {Audebert, Nicolas and Le Saux, Bertrand and Lefevre, Sebastien},
  date = {2017-07},
  pages = {1552--1560},
  publisher = {IEEE},
  location = {Honolulu, HI},
  doi = {10.1109/CVPRW.2017.199},
  url = {https://ieeexplore.ieee.org/document/8014933/},
  urldate = {2025-05-29},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}: {{Workshops}} ({{CVPRW}})},
  isbn = {978-1-5386-0733-6},
}

@article{BhatEtAlRobustlossfunctionclassimbalancedsemanticsegmentationimageclassification2023a,
  title = {Robust Loss Function for Class Imbalanced Semantic Segmentation and Image Classification},
  author = {Bhat, S Divakar and Amit, More and Soni, Mudit and Yasui, Yuji},
  date = {2023},
  journaltitle = {IFAC-PapersOnLine},
  volume = {56},
  number = {2},
  pages = {7934--7939},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2023.10.320},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2405896323006870},
  urldate = {2025-03-03},
  langid = {english},
}

@article{BoeingModelingAnalyzingUrbanNetworksAmenitiesOSMnx2025,
  title = {Modeling and {{Analyzing Urban Networks}} and {{Amenities With OSMnx}}},
  author = {Boeing, Geoff},
  date = {2025-05-03},
  journaltitle = {Geographical Analysis},
  pages = {gean.70009},
  issn = {0016-7363, 1538-4632},
  doi = {10.1111/gean.70009},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/gean.70009},
  urldate = {2025-06-25},
  abstract = {ABSTRACT             OSMnx is a Python package for downloading, modeling, analyzing, and visualizing urban networks and any other geospatial features from OpenStreetMap data. A large and growing body of literature uses it to conduct scientific studies across the disciplines of geography, urban planning, transport engineering, computer science, and others. The OSMnx project has recently developed and implemented many new features, modeling capabilities, and analytical methods. The package now encompasses substantially more functionality than was previously documented in the literature. This article introduces OSMnx's modern capabilities, usage, and design—in addition to the scientific theory and logic underlying them. It shares lessons learned in geospatial software development and reflects on open science's implications for urban modeling and analysis.},
  langid = {english},
  keywords = {/unread}
}

@online{BundesamtfurEich-undVermessungswesenOrthophotoFarbe2025,
  type = {Behördliche Website},
  title = {Orthophoto {{Farbe}}},
  author = {BundesamtfürEich-undVermessungswesen},
  date = {2025-06-23},
  url = {https://www.bev.gv.at/Services/Produkte/Luftbildprodukte/Orthophoto-Farbe.html},
  organization = {Orthophoto Farbe},
  keywords = {/unread}
}


@article{CostaEtAlSupervisedmethodsimagesegmentationaccuracyassessmentlandcovermapping2018,
  title = {Supervised Methods of Image Segmentation Accuracy Assessment in Land Cover Mapping},
  author = {Costa, Hugo and Foody, Giles M. and Boyd, Doreen S.},
  date = {2018-02},
  journaltitle = {Remote Sensing of Environment},
  volume = {205},
  pages = {338--351},
  issn = {00344257},
  doi = {10.1016/j.rse.2017.11.024},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425717305734},
  urldate = {2025-04-27},
  langid = {english},
}

@inproceedings{DengNewsamQuantitativeComparisonOpenSourceDataFineGrainMappingLandUse2017,
  title = {Quantitative {{Comparison}} of {{Open-Source Data}} for {{Fine-Grain Mapping}} of {{Land Use}}},
  author = {Deng, Xueqing and Newsam, Shawn},
  date = {2017-11-07},
  pages = {1--8},
  publisher = {ACM},
  location = {Redondo Beach CA USA},
  doi = {10.1145/3152178.3152182},
  url = {https://dl.acm.org/doi/10.1145/3152178.3152182},
  urldate = {2025-05-29},
  eventtitle = {{{SIGSPATIAL}}'17: 25th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  isbn = {978-1-4503-5495-0},
  langid = {english},
}

@article{DiakogiannisEtAlResUNetadeeplearningframeworksemanticsegmentationremotelysenseddata2020,
  title = {{{ResUNet-a}}: {{A}} Deep Learning Framework for Semantic Segmentation of Remotely Sensed Data},
  shorttitle = {{{ResUNet-a}}},
  author = {Diakogiannis, Foivos I. and Waldner, François and Caccetta, Peter and Wu, Chen},
  date = {2020-04},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {162},
  pages = {94--114},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2020.01.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271620300149},
  urldate = {2025-06-23},
  langid = {english},
}

@article{FengEtAlInformationLeakageDeepLearningBasedHyperspectralImageClassificationSurvey2023,
  title = {Information {{Leakage}} in {{Deep Learning-Based Hyperspectral Image Classification}}: {{A Survey}}},
  shorttitle = {Information {{Leakage}} in {{Deep Learning-Based Hyperspectral Image Classification}}},
  author = {Feng, Hao and Wang, Yongcheng and Li, Zheng and Zhang, Ning and Zhang, Yuxi and Gao, Yunxiao},
  date = {2023-07-30},
  journaltitle = {Remote Sensing},
  volume = {15},
  number = {15},
  pages = {3793},
  issn = {2072-4292},
  doi = {10.3390/rs15153793},
  url = {https://www.mdpi.com/2072-4292/15/15/3793},
  urldate = {2025-06-25},
  abstract = {In deep learning-based hyperspectral remote sensing image classification tasks, random sampling strategies are typically used to train model parameters for testing and evaluation. However, this approach leads to strong spatial autocorrelation between the training set samples and the surrounding test set samples, and some unlabeled test set data directly participate in the training of the network. This leaked information makes the model overly optimistic. Models trained under these conditions tend to overfit to a single dataset, which limits the range of practical applications. This paper analyzes the causes and effects of information leakage and summarizes the methods from existing models to mitigate the effects of information leakage. Specifically, this paper states the main issues in this area, where the issue of information leakage is addressed in detail. Second, some algorithms and related models used to mitigate information leakage are categorized, including reducing the number of training samples, using spatially disjoint sampling strategies, few-shot learning, and unsupervised learning. These models and methods are classified according to the sample-related phase and the feature extraction phase. Finally, several representative hyperspectral image classification models experiments are conducted on the common datasets and their effectiveness in mitigating information leakage is analyzed.},
  langid = {english},
}

@software{GilliesRasteriogeospatialrasterPythonprogrammers2013,
  title = {Rasterio: Geospatial Raster {{I}}/{{O}} for {{Python}} Programmers},
  author = {Gillies, Sean; and others },
  date = {2013},
  url = {https://github.com/rasterio/rasterio},
  organization = {Mapbox},
  keywords = {/unread}
}

@software{IakubovskiiSegmentationModelsPytorch2019,
  title = {Segmentation {{Models Pytorch}}},
  author = {Iakubovskii, Pavel},
  date = {2019},
  url = {https://github.com/qubvel/segmentation_models.pytorch},
  keywords = {/unread}
}

@inproceedings{Jadonsurveylossfunctionssemanticsegmentation2020,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  date = {2020-10-27},
  pages = {1--7},
  publisher = {IEEE},
  location = {Via del Mar, Chile},
  doi = {10.1109/CIBCB48159.2020.9277638},
  url = {https://ieeexplore.ieee.org/document/9277638/},
  urldate = {2025-06-25},
  eventtitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  isbn = {978-1-7281-9468-4},
}

@software{JordahlEtAlgeopandasgeopandasv0812020,
  title = {Geopandas/Geopandas: V0.8.1},
  shorttitle = {Geopandas/Geopandas},
  author = {Jordahl, Kelsey and Bossche, Joris Van Den and Fleischmann, Martin and Wasserman, Jacob and McBride, James and Gerard, Jeffrey and Tratner, Jeff and Perry, Matthew and Badaracco, Adrian Garcia and Farmer, Carson and Hjelle, Geir Arne and Snow, Alan D. and Cochran, Micah and Gillies, Sean and Culbertson, Lucas and Bartos, Matt and Eubank, Nick and {Maxalbert} and Bilogur, Aleksey and Rey, Sergio and Ren, Christopher and Arribas-Bel, Dani and Wasser, Leah and Wolf, Levi John and Journois, Martin and Wilson, Joshua and Greenhall, Adam and Holdgraf, Chris and Filipe and Leblanc, François},
  date = {2020-07-15},
  doi = {10.5281/ZENODO.3946761},
  url = {https://zenodo.org/record/3946761},
  urldate = {2025-06-25},
  abstract = {Small bug-fix release: Fix a regression in the {$<$}code{$>$}plot(){$<$}/code{$>$} method when visualizing with a JenksCaspallSampled or FisherJenksSampled scheme (\#1486). Fix spurious warning in {$<$}code{$>$}GeoDataFrame.to\_postgis{$<$}/code{$>$} (\#1497). Fix the un-pickling with {$<$}code{$>$}pd.read\_pickle{$<$}/code{$>$} of files written with older GeoPandas versions (\#1511). Thanks to Ian Rose, Joris Van den Bossche and Martin Fleischmann for their contributions!},
  organization = {Zenodo},
  version = {v0.8.1},
  keywords = {/unread}
}

@article{KaiserEtAlLearningAerialImageSegmentationOnlineMaps2017,
  title = {Learning {{Aerial Image Segmentation From Online Maps}}},
  author = {Kaiser, Pascal and Wegner, Jan Dirk and Lucchi, Aurelien and Jaggi, Martin and Hofmann, Thomas and Schindler, Konrad},
  date = {2017-11},
  journaltitle = {IEEE Trans. Geosci. Remote Sensing},
  volume = {55},
  number = {11},
  pages = {6054--6068},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2017.2719738},
  url = {http://ieeexplore.ieee.org/document/7987710/},
  urldate = {2025-05-30},
  abstract = {This paper deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps that can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.},
  langid = {english},
}

@article{KandzioraEtAlMappingprovisioningecosystemserviceslocalscaleusingdatavaryingspatialtemporalresolution2013,
  title = {Mapping Provisioning Ecosystem Services at the Local Scale Using Data of Varying Spatial and Temporal Resolution},
  author = {Kandziora, Marion and Burkhard, Benjamin and Müller, Felix},
  date = {2013-06},
  journaltitle = {Ecosystem Services},
  volume = {4},
  pages = {47--59},
  issn = {22120416},
  doi = {10.1016/j.ecoser.2013.04.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2212041613000211},
  urldate = {2025-06-23},
  langid = {english},
}

@article{KotaridisLazaridouRemotesensingimagesegmentationadvancesmetaanalysis2021a,
  title = {Remote Sensing Image Segmentation Advances: {{A}} Meta-Analysis},
  shorttitle = {Remote Sensing Image Segmentation Advances},
  author = {Kotaridis, Ioannis and Lazaridou, Maria},
  date = {2021-03},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {173},
  pages = {309--322},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2021.01.020},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271621000265},
  urldate = {2025-04-05},
  abstract = {The advances in remote sensing sensors during the last two decades have led to the production of very high spatial resolution multispectral images. In order to adapt to this rapid development and handle these data, object-based analysis has emerged. A critical part of such an analysis is image segmentation. The selection of optimal segmentation parameters’ values generates a qualitative segmentation output and has a direct impact on feature extraction and subsequent overall classification accuracy. Even though several image segmentation methods have been developed and suggested in the literature, each of them has advantages and disadvantages. This article presents the conceptual characteristics of image segmentation methods with a special focus on semantic segmentation. In addition, a meta-analysis was conducted through a comprehensive review of recent image segmentation case studies. It includes statistics and quantitative data regarding the applied segmentation algorithm, the software utilized and the data source among others. Since there is no miraculous segmentation algorithm, the statistical results depict only the recent trend. Finally, a few interesting subjects are addressed, including identification of current problems, image segmentation on non-traditional data and hot topics for future research.},
  langid = {english},
}

@article{LeiEtAlDeeplearningimplementationimagesegmentationagriculturalapplicationscomprehensivereview2024,
  title = {Deep Learning Implementation of Image Segmentation in Agricultural Applications: A Comprehensive Review},
  shorttitle = {Deep Learning Implementation of Image Segmentation in Agricultural Applications},
  author = {Lei, Lian and Yang, Qiliang and Yang, Ling and Shen, Tao and Wang, Ruoxi and Fu, Chengbiao},
  date = {2024-05-22},
  journaltitle = {Artif Intell Rev},
  volume = {57},
  number = {6},
  pages = {149},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-10775-6},
  url = {https://link.springer.com/10.1007/s10462-024-10775-6},
  urldate = {2025-01-18},
  abstract = {Abstract             Image segmentation is a crucial task in computer vision, which divides a digital image into multiple segments and objects. In agriculture, image segmentation is extensively used for crop and soil monitoring, predicting the best times to sow, fertilize, and harvest, estimating crop yield, and detecting plant diseases. However, image segmentation faces difficulties in agriculture, such as the challenges of disease staging recognition, labeling inconsistency, and changes in plant morphology with the environment. Consequently, we have conducted a comprehensive review of image segmentation techniques based on deep learning, exploring the development and prospects of image segmentation in agriculture. Deep learning-based image segmentation solutions widely used in agriculture are categorized into eight main groups: encoder-decoder structures, multi-scale and pyramid-based methods, dilated convolutional networks, visual attention models, generative adversarial networks, graph neural networks, instance segmentation networks, and transformer-based models. In addition, the applications of image segmentation methods in agriculture are presented, such as plant disease detection, weed identification, crop growth monitoring, crop yield estimation, and counting. Furthermore, a collection of publicly available plant image segmentation datasets has been reviewed, and the evaluation and comparison of performance for image segmentation algorithms have been conducted on benchmark datasets. Finally, there is a discussion of the challenges and future prospects of image segmentation in agriculture.},
  langid = {english},
}

@article{LoshchilovHutterDecoupledWeightDecayRegularization2017,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017},
  doi = {10.48550/ARXIV.1711.05101},
  url = {https://arxiv.org/abs/1711.05101},
  urldate = {2025-06-25},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  version = {3},
  keywords = {/unread,FOS: Computer and information sciences,FOS: Mathematics,Machine Learning (cs.LG),Neural and Evolutionary Computing (cs.NE),Optimization and Control (math.OC)},
}

@article{LuoEtAlSemanticsegmentationagriculturalimagessurvey2024,
  title = {Semantic Segmentation of Agricultural Images: {{A}} Survey},
  shorttitle = {Semantic Segmentation of Agricultural Images},
  author = {Luo, Zifei and Yang, Wenzhu and Yuan, Yunfeng and Gou, Ruru and Li, Xiaonan},
  date = {2024-06},
  journaltitle = {Information Processing in Agriculture},
  volume = {11},
  number = {2},
  pages = {172--186},
  issn = {22143173},
  doi = {10.1016/j.inpa.2023.02.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2214317323000112},
  urldate = {2025-03-21},
  abstract = {As an important research topic in recent years, semantic segmentation has been widely applied to image understanding problems in various fields. With the successful application of deep learning methods in machine vision, the superior performance has been transferred to agricultural image processing by combining them with traditional methods. Semantic segmentation methods have revolutionized the development of agricultural automation and are commonly used for crop cover and type analysis, pest and disease identification, etc. We first give a review of the recent advances in traditional and deep learning methods for semantic segmentation of agricultural images according to different segmentation principles. Then we introduce the traditional methods that can effectively utilize the original image information and the powerful performance of deep learningbased methods. Finally, we outline their applications in agricultural image segmentation. In our literature, we identify the challenges in agricultural image segmentation and summarize the innovative developments that address these challenges. The robustness of the existing segmentation methods for processing complex images still needs to be improved urgently, and their generalization abilities are also insufficient. In particular, the limited number of labeled samples is a roadblock to new developed deep learning methods for their training and evaluation. To this, segmentation methods that augment the dataset or incorporate multimodal information enable deep learning methods to further improve the segmentation capabilities. This review provides a reference for the application of image semantic segmentation in the field of agricultural informatization.},
  langid = {english},
}

@article{LvEtAlDeeplearningbasedsemanticsegmentationremotesensingimagesreview2023,
  title = {Deep Learning-Based Semantic Segmentation of Remote Sensing Images: A Review},
  shorttitle = {Deep Learning-Based Semantic Segmentation of Remote Sensing Images},
  author = {Lv, Jinna and Shen, Qi and Lv, Mingzheng and Li, Yiran and Shi, Lei and Zhang, Peiying},
  date = {2023-07-14},
  journaltitle = {Front. Ecol. Evol.},
  volume = {11},
  pages = {1201125},
  issn = {2296-701X},
  doi = {10.3389/fevo.2023.1201125},
  url = {https://www.frontiersin.org/articles/10.3389/fevo.2023.1201125/full},
  urldate = {2025-06-19},
  abstract = {Semantic segmentation is a fundamental but challenging problem of pixel-level remote sensing (RS) data analysis. Semantic segmentation tasks based on aerial and satellite images play an important role in a wide range of applications. Recently, with the successful applications of deep learning (DL) in the computer vision (CV) field, more and more researchers have introduced and improved DL methods to the task of RS data semantic segmentation and achieved excellent results. Although there are a large number of DL methods, there remains a deficiency in the evaluation and advancement of semantic segmentation techniques for RS data. To solve the problem, this paper surveys more than 100 papers in this field in the past 5 years and elaborates in detail on the aspects of technical framework classification discussion, datasets, experimental evaluation, research challenges, and future research directions. Different from several previously published surveys, this paper first focuses on comprehensively summarizing the advantages and disadvantages of techniques and models based on the important and difficult points. This research will help beginners quickly establish research ideas and processes in this field, allowing them to focus on algorithm innovation without paying too much attention to datasets, evaluation indicators, and research frameworks.},
}

@article{MarmanisEtAlSEMANTICSEGMENTATIONAERIALIMAGESENSEMBLECNNS,
  title = {{{SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH AN ENSEMBLE OF CNNS}}},
  author = {Marmanis, D and Wegner, J D and Galliani, S and Schindler, K and Datcu, M and Stilla, U},
  abstract = {This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images. Deep neural architectures hold the promise of end-to-end learning from raw images, making heuristic feature design obsolete. Over the last decade this idea has seen a revival, and in recent years deep convolutional neural networks (CNNs) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection. Still, standard CNNs do not lend themselves to per-pixel semantic segmentation, mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions, making it hard to disentangle contributions from different pixels. Very recently two extensions of the CNN framework have made it possible to trace the semantic information back to a precise pixel position: deconvolutional network layers undo the spatial downsampling, and Fully Convolution Networks (FCNs) modify the fully connected classification layers of the network in such a way that the location of individual activations remains explicit. We design a FCN which takes as input intensity and range data and, with the help of aggressive deconvolution and recycling of early network layers, converts them into a pixelwise classification at full resolution. We discuss design choices and intricacies of such a network, and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the ISPRS semantic labeling benchmark, using only the raw data as input.},
  langid = {english},
  keywords = {/unread},
}

@article{NeupaneEtAlDeepLearningBasedSemanticSegmentationUrbanFeaturesSatelliteImagesReviewMetaAnalysis2021,
  title = {Deep {{Learning-Based Semantic Segmentation}} of {{Urban Features}} in {{Satellite Images}}: {{A Review}} and {{Meta-Analysis}}},
  shorttitle = {Deep {{Learning-Based Semantic Segmentation}} of {{Urban Features}} in {{Satellite Images}}},
  author = {Neupane, Bipul and Horanont, Teerayut and Aryal, Jagannath},
  date = {2021-02-23},
  journaltitle = {Remote Sensing},
  volume = {13},
  number = {4},
  pages = {808},
  issn = {2072-4292},
  doi = {10.3390/rs13040808},
  url = {https://www.mdpi.com/2072-4292/13/4/808},
  urldate = {2025-03-04},
  abstract = {Availability of very high-resolution remote sensing images and advancement of deep learning methods have shifted the paradigm of image classification from pixel-based and object-based methods to deep learning-based semantic segmentation. This shift demands a structured analysis and revision of the current status on the research domain of deep learning-based semantic segmentation. The focus of this paper is on urban remote sensing images. We review and perform a meta-analysis to juxtapose recent papers in terms of research problems, data source, data preparation methods including pre-processing and augmentation techniques, training details on architectures, backbones, frameworks, optimizers, loss functions and other hyper-parameters and performance comparison. Our detailed review and meta-analysis show that deep learning not only outperforms traditional methods in terms of accuracy, but also addresses several challenges previously faced. Further, we provide future directions of research in this domain.},
  langid = {english},
}

@dataset{OpenStreetMapcontributorsOpenStreetMap2025,
  title = {{{OpenStreetMap}}},
  author = {OpenStreetMapContributors},
  date = {2025},
  location = {Overpass API},
  keywords = {/unread}
}

@inproceedings{OrfanidisEtAlDeepNeuralNetworkOilSpillSemanticSegmentationSarImages2018,
  title = {A {{Deep Neural Network}} for {{Oil Spill Semantic Segmentation}} in {{Sar Images}}},
  booktitle = {2018 25th {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Orfanidis, Georgios and Ioannidis, Konstantinos and Avgerinakis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  date = {2018-10},
  pages = {3773--3777},
  publisher = {IEEE},
  location = {Athens},
  doi = {10.1109/ICIP.2018.8451113},
  url = {https://ieeexplore.ieee.org/document/8451113/},
  urldate = {2025-06-25},
  eventtitle = {2018 25th {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  isbn = {978-1-4799-7061-2},
}

@article{PandeyEtAlLanduselandcoverviewearthobservationdatasourcesinputdimensionsclassifiersreviewstateart2021,
  title = {Land Use/Land Cover in View of Earth Observation: Data Sources, Input Dimensions, and Classifiers—a Review of the State of the Art},
  shorttitle = {Land Use/Land Cover in View of Earth Observation},
  author = {Pandey, Prem Chandra and Koutsias, Nikos and Petropoulos, George P. and Srivastava, Prashant K. and Ben Dor, Eyal},
  date = {2021-05-28},
  journaltitle = {Geocarto International},
  volume = {36},
  number = {9},
  pages = {957--988},
  issn = {1010-6049, 1752-0762},
  doi = {10.1080/10106049.2019.1629647},
  url = {https://www.tandfonline.com/doi/full/10.1080/10106049.2019.1629647},
  urldate = {2025-06-19},
  langid = {english},
}

@article{PashaeiEtAlReviewEvaluationDeepLearningArchitecturesEfficientLandCoverMappingUASHyperSpatialImageryCaseStudyWetland2020,
  title = {Review and {{Evaluation}} of {{Deep Learning Architectures}} for {{Efficient Land Cover Mapping}} with {{UAS Hyper-Spatial Imagery}}: {{A Case Study Over}} a {{Wetland}}},
  shorttitle = {Review and {{Evaluation}} of {{Deep Learning Architectures}} for {{Efficient Land Cover Mapping}} with {{UAS Hyper-Spatial Imagery}}},
  author = {Pashaei, Mohammad and Kamangir, Hamid and Starek, Michael J. and Tissot, Philippe},
  date = {2020-03-16},
  journaltitle = {Remote Sensing},
  volume = {12},
  number = {6},
  pages = {959},
  issn = {2072-4292},
  doi = {10.3390/rs12060959},
  url = {https://www.mdpi.com/2072-4292/12/6/959},
  urldate = {2025-06-19},
  abstract = {Deep learning has already been proved as a powerful state-of-the-art technique for many image understanding tasks in computer vision and other applications including remote sensing (RS) image analysis. Unmanned aircraft systems (UASs) offer a viable and economical alternative to a conventional sensor and platform for acquiring high spatial and high temporal resolution data with high operational flexibility. Coastal wetlands are among some of the most challenging and complex ecosystems for land cover prediction and mapping tasks because land cover targets often show high intra-class and low inter-class variances. In recent years, several deep convolutional neural network (CNN) architectures have been proposed for pixel-wise image labeling, commonly called semantic image segmentation. In this paper, some of the more recent deep CNN architectures proposed for semantic image segmentation are reviewed, and each model’s training efficiency and classification performance are evaluated by training it on a limited labeled image set. Training samples are provided using the hyper-spatial resolution UAS imagery over a wetland area and the required ground truth images are prepared by manual image labeling. Experimental results demonstrate that deep CNNs have a great potential for accurate land cover prediction task using UAS hyper-spatial resolution images. Some simple deep learning architectures perform comparable or even better than complex and very deep architectures with remarkably fewer training epochs. This performance is especially valuable when limited training samples are available, which is a common case in most RS applications.},
  langid = {english},
}

@article{PiresDeLimaMarfurtConvolutionalNeuralNetworkRemoteSensingSceneClassificationTransferLearningAnalysis2019,
  title = {Convolutional {{Neural Network}} for {{Remote-Sensing Scene Classification}}: {{Transfer Learning Analysis}}},
  shorttitle = {Convolutional {{Neural Network}} for {{Remote-Sensing Scene Classification}}},
  author = {Pires De Lima, Rafael and Marfurt, Kurt},
  date = {2019-12-25},
  journaltitle = {Remote Sensing},
  volume = {12},
  number = {1},
  pages = {86},
  issn = {2072-4292},
  doi = {10.3390/rs12010086},
  url = {https://www.mdpi.com/2072-4292/12/1/86},
  urldate = {2025-06-25},
  abstract = {Remote-sensing image scene classification can provide significant value, ranging from forest fire monitoring to land-use and land-cover classification. Beginning with the first aerial photographs of the early 20th century to the satellite imagery of today, the amount of remote-sensing data has increased geometrically with a higher resolution. The need to analyze these modern digital data motivated research to accelerate remote-sensing image classification. Fortunately, great advances have been made by the computer vision community to classify natural images or photographs taken with an ordinary camera. Natural image datasets can range up to millions of samples and are, therefore, amenable to deep-learning techniques. Many fields of science, remote sensing included, were able to exploit the success of natural image classification by convolutional neural network models using a technique commonly called transfer learning. We provide a systematic review of transfer learning application for scene classification using different datasets and different deep-learning models. We evaluate how the specialization of convolutional neural network models affects the transfer learning process by splitting original models in different points. As expected, we find the choice of hyperparameters used to train the model has a significant influence on the final performance of the models. Curiously, we find transfer learning from models trained on larger, more generic natural images datasets outperformed transfer learning from models trained directly on smaller remotely sensed datasets. Nonetheless, results show that transfer learning provides a powerful tool for remote-sensing scene classification.},
  langid = {english},
}

@article{QinLiuReviewLandcoverClassificationVeryHighResolutionRemotelySensedOpticalImagesAnalysisUnitModelScalabilityTransferability2022,
  title = {A {{Review}} of {{Landcover Classification}} with {{Very-High Resolution Remotely Sensed Optical Images}}—{{Analysis Unit}}, {{Model Scalability}} and {{Transferability}}},
  author = {Qin, Rongjun and Liu, Tao},
  date = {2022-01-29},
  journaltitle = {Remote Sensing},
  volume = {14},
  number = {3},
  pages = {646},
  issn = {2072-4292},
  doi = {10.3390/rs14030646},
  url = {https://www.mdpi.com/2072-4292/14/3/646},
  urldate = {2025-06-19},
  abstract = {As an important application in remote sensing, landcover classification remains one of the most challenging tasks in very-high-resolution (VHR) image analysis. As the rapidly increasing number of Deep Learning (DL) based landcover methods and training strategies are claimed to be the state-of-the-art, the already fragmented technical landscape of landcover mapping methods has been further complicated. Although there exists a plethora of literature review work attempting to guide researchers in making an informed choice of landcover mapping methods, the articles either focus on the review of applications in a specific area or revolve around general deep learning models, which lack a systematic view of the ever advancing landcover mapping methods. In addition, issues related to training samples and model transferability have become more critical than ever in an era dominated by data-driven approaches, but these issues were addressed to a lesser extent in previous review articles regarding remote sensing classification. Therefore, in this paper, we present a systematic overview of existing methods by starting from learning methods and varying basic analysis units for landcover mapping tasks, to challenges and solutions on three aspects of scalability and transferability with a remote sensing classification focus including (1) sparsity and imbalance of data; (2) domain gaps across different geographical regions; and (3) multi-source and multi-view fusion. We discuss in detail each of these categorical methods and draw concluding remarks in these developments and recommend potential directions for the continued endeavor.},
  langid = {english},
}

@inproceedings{RezatofighiEtAlGeneralizedIntersectionUnionMetricLossBoundingBoxRegression2019,
  title = {Generalized {{Intersection Over Union}}: {{A Metric}} and a {{Loss}} for {{Bounding Box Regression}}},
  shorttitle = {Generalized {{Intersection Over Union}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  date = {2019-06},
  pages = {658--666},
  publisher = {IEEE},
  location = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00075},
  url = {https://ieeexplore.ieee.org/document/8953982/},
  urldate = {2025-06-25},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-7281-3293-8},
}

@online{RonnebergerEtAlUNetConvolutionalNetworksBiomedicalImageSegmentation2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015},
  doi = {10.48550/ARXIV.1505.04597},
  url = {https://arxiv.org/abs/1505.04597},
  urldate = {2025-06-25},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  pubstate = {prepublished},
  version = {1},
  keywords = {/unread,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
 }

@article{SertelEtAlLandUseLandCoverMappingUsingDeepLearningBasedSegmentationApproachesVHRWorldview3Images2022,
  title = {Land {{Use}} and {{Land Cover Mapping Using Deep Learning Based Segmentation Approaches}} and {{VHR Worldview-3 Images}}},
  author = {Sertel, Elif and Ekim, Burak and Ettehadi Osgouei, Paria and Kabadayi, M. Erdem},
  date = {2022-09-12},
  journaltitle = {Remote Sensing},
  volume = {14},
  number = {18},
  pages = {4558},
  issn = {2072-4292},
  doi = {10.3390/rs14184558},
  url = {https://www.mdpi.com/2072-4292/14/18/4558},
  urldate = {2025-05-29},
  abstract = {Deep learning-based segmentation of very high-resolution (VHR) satellite images is a significant task providing valuable information for various geospatial applications, specifically for land use/land cover (LULC) mapping. The segmentation task becomes more challenging with the increasing number and complexity of LULC classes. In this research, we generated a new benchmark dataset from VHR Worldview-3 images for twelve distinct LULC classes of two different geographical locations. We evaluated the performance of different segmentation architectures and encoders to find the best design to create highly accurate LULC maps. Our results showed that the DeepLabv3+ architecture with an ResNeXt50 encoder achieved the best performance for different metric values with an IoU of 89.46\%, an F-1 score of 94.35\%, a precision of 94.25\%, and a recall of 94.49\%. This design could be used by other researchers for LULC mapping of similar classes from different satellite images or for different geographical regions. Moreover, our benchmark dataset can be used as a reference for implementing new segmentation models via supervised, semi- or weakly-supervised deep learning models. In addition, our model results can be used for transfer learning and generalizability of different methodologies.},
  langid = {english},
}

@article{StewartEtAlTorchGeoDeepLearningGeospatialData2024,
  title = {{{TorchGeo}}: {{Deep Learning With Geospatial Data}}},
  shorttitle = {{{TorchGeo}}},
  author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Lavista Ferres, Juan M. and Banerjee, Arindam},
  date = {2024-12-16},
  journaltitle = {ACM Trans. Spatial Algorithms Syst.},
  pages = {3707459},
  issn = {2374-0353, 2374-0361},
  doi = {10.1145/3707459},
  url = {https://dl.acm.org/doi/10.1145/3707459},
  urldate = {2025-06-25},
  abstract = {Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that may have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for uncurated geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.},
  langid = {english},
}

@article{TalukdarEtAlLandUseLandCoverClassificationMachineLearningClassifiersSatelliteObservationsReview2020,
  title = {Land-{{Use Land-Cover Classification}} by {{Machine Learning Classifiers}} for {{Satellite Observations}}—{{A Review}}},
  author = {Talukdar, Swapan and Singha, Pankaj and Mahato, Susanta and {Shahfahad} and Pal, Swades and Liou, Yuei-An and Rahman, Atiqur},
  date = {2020-04-02},
  journaltitle = {Remote Sensing},
  volume = {12},
  number = {7},
  pages = {1135},
  issn = {2072-4292},
  doi = {10.3390/rs12071135},
  url = {https://www.mdpi.com/2072-4292/12/7/1135},
  urldate = {2025-06-22},
  abstract = {Rapid and uncontrolled population growth along with economic and industrial development, especially in developing countries during the late twentieth and early twenty-first centuries, have increased the rate of land-use/land-cover (LULC) change many times. Since quantitative assessment of changes in LULC is one of the most efficient means to understand and manage the land transformation, there is a need to examine the accuracy of different algorithms for LULC mapping in order to identify the best classifier for further applications of earth observations. In this article, six machine-learning algorithms, namely random forest (RF), support vector machine (SVM), artificial neural network (ANN), fuzzy adaptive resonance theory-supervised predictive mapping (Fuzzy ARTMAP), spectral angle mapper (SAM) and Mahalanobis distance (MD) were examined. Accuracy assessment was performed by using Kappa coefficient, receiver operational curve (RoC), index-based validation and root mean square error (RMSE). Results of Kappa coefficient show that all the classifiers have a similar accuracy level with minor variation, but the RF algorithm has the highest accuracy of 0.89 and the MD algorithm (parametric classifier) has the least accuracy of 0.82. In addition, the index-based LULC and visual cross-validation show that the RF algorithm (correlations between RF and normalised differentiation water index, normalised differentiation vegetation index and normalised differentiation built-up index are 0.96, 0.99 and 1, respectively, at 0.05 level of significance) has the highest accuracy level in comparison to the other classifiers adopted. Findings from the literature also proved that ANN and RF algorithms are the best LULC classifiers, although a non-parametric classifier like SAM (Kappa coefficient 0.84; area under curve (AUC) 0.85) has a better and consistent accuracy level than the other machine-learning algorithms. Finally, this review concludes that the RF algorithm is the best machine-learning LULC classifier, among the six examined algorithms although it is necessary to further test the RF algorithm in different morphoclimatic conditions in the future.},
  langid = {english},
}

@article{UsmaniEtAlglobalscalesegmentationOpenStreetMapremotesensing2023,
  title = {Towards Global Scale Segmentation with {{OpenStreetMap}} and Remote Sensing},
  author = {Usmani, Munazza and Napolitano, Maurizio and Bovolo, Francesca},
  date = {2023-04},
  journaltitle = {ISPRS Open Journal of Photogrammetry and Remote Sensing},
  volume = {8},
  pages = {100031},
  issn = {26673932},
  doi = {10.1016/j.ophoto.2023.100031},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2667393223000029},
  urldate = {2025-06-19},
  langid = {english},
}

@article{UsmaniEtAlRemoteSensingDeepLearningUnderstandNoisyOpenStreetMap2023,
  title = {Remote {{Sensing}} and {{Deep Learning}} to {{Understand Noisy OpenStreetMap}}},
  author = {Usmani, Munazza and Bovolo, Francesca and Napolitano, Maurizio},
  date = {2023-09-21},
  journaltitle = {Remote Sensing},
  volume = {15},
  number = {18},
  pages = {4639},
  issn = {2072-4292},
  doi = {10.3390/rs15184639},
  url = {https://www.mdpi.com/2072-4292/15/18/4639},
  urldate = {2025-06-23},
  abstract = {The OpenStreetMap (OSM) project is an open-source, community-based, user-generated street map/data service. It is the most popular project within the state of the art for crowdsourcing. Although geometrical features and tags of annotations in OSM are usually precise (particularly in metropolitan areas), there are instances where volunteer mapping is inaccurate. Despite the appeal of using OSM semantic information with remote sensing images, to train deep learning models, the crowdsourced data quality is inconsistent. High-resolution remote sensing image segmentation is a mature application in many fields, such as urban planning, updated mapping, city sensing, and others. Typically, supervised methods trained with annotated data may learn to anticipate the object location, but misclassification may occur due to noise in training data. This article combines Very High Resolution (VHR) remote sensing data with computer vision methods to deal with noisy OSM. This work deals with OSM misalignment ambiguity (positional inaccuracy) concerning satellite imagery and uses a Convolutional Neural Network (CNN) approach to detect missing buildings in OSM. We propose a translating method to align the OSM vector data with the satellite data. This strategy increases the correlation between the imagery and the building vector data to reduce the noise in OSM data. A series of experiments demonstrate that our approach plays a significant role in (1) resolving the misalignment issue, (2) instance-semantic segmentation of buildings with missing building information in OSM (never labeled or constructed in between image acquisitions), and (3) change detection mapping. The good results of precision (0.96) and recall (0.96) demonstrate the viability of high-resolution satellite imagery and OSM for building detection/change detection using a deep learning approach.},
  langid = {english},
}

@article{WangEtAlComprehensiveSurveyLossFunctionsMachineLearning2022,
  title = {A {{Comprehensive Survey}} of {{Loss Functions}} in {{Machine Learning}}},
  author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
  date = {2022-04},
  journaltitle = {Ann. Data. Sci.},
  volume = {9},
  number = {2},
  pages = {187--212},
  issn = {2198-5804, 2198-5812},
  doi = {10.1007/s40745-020-00253-5},
  url = {https://link.springer.com/10.1007/s40745-020-00253-5},
  urldate = {2025-06-25},
  langid = {english},
}

@article{XieEtAlSegFormerSimpleEfficientDesignSemanticSegmentationTransformers2021,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  shorttitle = {{{SegFormer}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
  date = {2021},
  doi = {10.48550/ARXIV.2105.15203},
  url = {https://arxiv.org/abs/2105.15203},
  urldate = {2025-07-17},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
  pubstate = {prepublished},
  version = {3},
  keywords = {/unread,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
}

@article{XieEtAlTransferLearningDeepFeaturesRemoteSensingPovertyMapping2016,
  title = {Transfer {{Learning}} from {{Deep Features}} for {{Remote Sensing}} and {{Poverty Mapping}}},
  author = {Xie, Michael and Jean, Neal and Burke, Marshall and Lobell, David and Ermon, Stefano},
  date = {2016-03-05},
  journaltitle = {AAAI},
  volume = {30},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v30i1.9906},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/9906},
  urldate = {2025-06-25},
  abstract = {The lack of reliable data in developing countries is a major obstacle to sustainable development, food security, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming increasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no techniques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian efforts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Networks (CNN). We therefore propose a transfer learning approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are useful for poverty prediction. The model learns filters identifying different terrains and man-made structures, including roads, buildings, and farmlands, without any supervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field.},
  keywords = {/unread},
}

@article{XuEtAlsemanticsegmentationmethodcategoryboundaryLandUseLandCoverLULCmappingVeryHighResolutionVHRremotesensingimage2021,
  title = {A Semantic Segmentation Method with Category Boundary for {{Land Use}} and {{Land Cover}} ({{LULC}}) Mapping of {{Very-High Resolution}} ({{VHR}}) Remote Sensing Image},
  author = {Xu, Zeyu and Su, Cheng and Zhang, Xiaocan},
  date = {2021-04-18},
  journaltitle = {International Journal of Remote Sensing},
  volume = {42},
  number = {8},
  pages = {3146--3165},
  issn = {0143-1161, 1366-5901},
  doi = {10.1080/01431161.2020.1871100},
  url = {https://www.tandfonline.com/doi/full/10.1080/01431161.2020.1871100},
  urldate = {2025-06-19},
  langid = {english},
}

@article{YangEtAlWaterSegformerlightweightmodelwaterbodyinformationextractionremotesensingimages2023,
  title = {{{WaterSegformer}}: {{A}} Lightweight Model for Water Body Information Extraction from Remote Sensing Images},
  shorttitle = {{{WaterSegformer}}},
  author = {Yang, Xiao and Chen, Mingwei and Yu, Chengjun and Huang, Haozhe and Yue, Xiaobin and Zhou, Bei and Ni, Ming},
  date = {2023-02},
  journaltitle = {IET Image Processing},
  volume = {17},
  number = {3},
  pages = {862--871},
  issn = {1751-9659, 1751-9667},
  doi = {10.1049/ipr2.12678},
  url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.12678},
  urldate = {2025-06-25},
  abstract = {Abstract             Accurate and efficient extraction of water body information from remote sensing images is of great help to monitor water resources at the macro level, natural disaster prediction, and water pollution detection and prevention. Although many large models have achieved extremely high accuracy in remote sensing image water segmentation tasks, lightweight models are still a non‐negligible choice for many application scenarios because of the limitation of computing and storage resources. Here, WaterSegformer is described, an efficient and powerful lightweight water body segmentation model based on Segformer‐b0. The Deepmask module is designed to make the model pay more attention to the details in the image and use Lovász loss to improve IoU. In addition, DeepLabv3+ is used as the teacher model to guide the training of the model in the way of relational knowledge distillation. WaterSegformer realizes 95.06\% mIoU on the test set with only 6.38 G and 3.72~M of FLOPs and parameters, respectively. Experimental results show that WaterSegformer achieves an excellent balance between accuracy, computational complexity and model size, which is hardware‐friendly, easy to deploy and enables real‐time segmentation. This method provides a new idea for water body information extraction from remote sensing images in practical applications.},
  langid = {english},
}

@article{YuanEtAlreviewdeeplearningmethodssemanticsegmentationremotesensingimagery2021,
  title = {A Review of Deep Learning Methods for Semantic Segmentation of Remote Sensing Imagery},
  author = {Yuan, Xiaohui and Shi, Jianfang and Gu, Lichuan},
  date = {2021-05},
  journaltitle = {Expert Systems with Applications},
  volume = {169},
  pages = {114417},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.114417},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420310836},
  urldate = {2025-06-19},
  langid = {english},
}

@article{ZangEtAlLandUseMappingHighSpatialResolutionRemoteSensingImageDeepLearningReview2021,
  title = {Land-{{Use Mapping}} for {{High-Spatial Resolution Remote Sensing Image Via Deep Learning}}: {{A Review}}},
  shorttitle = {Land-{{Use Mapping}} for {{High-Spatial Resolution Remote Sensing Image Via Deep Learning}}},
  author = {Zang, Ning and Cao, Yun and Wang, Yuebin and Huang, Bo and Zhang, Liqiang and Mathiopoulos, P. Takis},
  date = {2021},
  journaltitle = {IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing},
  volume = {14},
  pages = {5372--5391},
  issn = {1939-1404, 2151-1535},
  doi = {10.1109/JSTARS.2021.3078631},
  url = {https://ieeexplore.ieee.org/document/9427218/},
  urldate = {2025-06-19},
}

@article{ZhaoEtAlLandUseLandCoverClassificationMeetsDeepLearningReview2023,
  title = {Land {{Use}} and {{Land Cover Classification Meets Deep Learning}}: {{A Review}}},
  shorttitle = {Land {{Use}} and {{Land Cover Classification Meets Deep Learning}}},
  author = {Zhao, Shengyu and Tu, Kaiwen and Ye, Shutong and Tang, Hao and Hu, Yaocong and Xie, Chao},
  date = {2023-11-03},
  journaltitle = {Sensors},
  volume = {23},
  number = {21},
  pages = {8966},
  issn = {1424-8220},
  doi = {10.3390/s23218966},
  url = {https://www.mdpi.com/1424-8220/23/21/8966},
  urldate = {2025-06-19},
  abstract = {As one of the important components of Earth observation technology, land use and land cover (LULC) image classification plays an essential role. It uses remote sensing techniques to classify specific categories of ground cover as a means of analyzing and understanding the natural attributes of the Earth’s surface and the state of land use. It provides important information for applications in environmental protection, urban planning, and land resource management. However, remote sensing images are usually high-dimensional data and have limited available labeled samples, so performing the LULC classification task faces great challenges. In recent years, due to the emergence of deep learning technology, remote sensing data processing methods based on deep learning have achieved remarkable results, bringing new possibilities for the research and development of LULC classification. In this paper, we present a systematic review of deep-learning-based LULC classification, mainly covering the following five aspects: (1) introduction of the main components of five typical deep learning networks, how they work, and their unique benefits; (2) summary of two baseline datasets for LULC classification (pixel-level, patch-level) and performance metrics for evaluating different models (OA, AA, F1, and MIOU); (3) review of deep learning strategies in LULC classification studies, including convolutional neural networks (CNNs), autoencoders (AEs), generative adversarial networks (GANs), and recurrent neural networks (RNNs); (4) challenges faced by LULC classification and processing schemes under limited training samples; (5) outlooks on the future development of deep-learning-based LULC classification.},
  langid = {english},
}

@article{ZhongEtAlDeeplearningbasedmultitemporalcropclassification2019,
  title = {Deep Learning Based Multi-Temporal Crop Classification},
  author = {Zhong, Liheng and Hu, Lina and Zhou, Hang},
  date = {2019-02},
  journaltitle = {Remote Sensing of Environment},
  volume = {221},
  pages = {430--443},
  issn = {00344257},
  doi = {10.1016/j.rse.2018.11.032},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425718305418},
  urldate = {2025-06-25},
  langid = {english},
  keywords = {/unread},
}